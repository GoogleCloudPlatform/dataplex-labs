#!/usr/bin/env python3
"""
import_glossaries_from_csv.py

Sequentially imports the CSVs generated by bulk_create_datacatalog_glossaries.py
(using the mapping written to ./datasets/_created_glossaries.txt).

Each line in _created_glossaries.txt should be:
    <entry_group_id>,<glossary_id>

For each mapping we will call business_glossary_import.py with the correct
--group and --glossary values.
"""

import subprocess
import os
import sys
import argparse
from pathlib import Path
import time
import shlex
from concurrent.futures import ThreadPoolExecutor, as_completed

DEFAULT_OUTPUT_ROOT = "./datasets"
DEFAULT_PROJECT = "project-with-100-glossaries"
DEFAULT_LOCATION = "us"
SLEEP_BETWEEN = 0.5  # seconds

def get_access_token():
    try:
        out = subprocess.check_output(["gcloud", "auth", "print-access-token"], stderr=subprocess.STDOUT)
        return out.decode().strip()
    except subprocess.CalledProcessError as e:
        print("Failed to get access token:", e.output.decode(), file=sys.stderr)
        raise

def run_import_for_glossary(project, location, entry_group, glossary, categories_csv, terms_csv, timeout=300):
    env = os.environ.copy()
    token = get_access_token()
    env["GCLOUD_ACCESS_TOKEN"] = token

    cmd = [
        sys.executable,
        "business_glossary_import.py",
        f"--project={project}",
        f"--group={entry_group}",
        f"--glossary={glossary}",
        f"--location={location}",
        "--import-mode=clear",
        f"--categories-csv={categories_csv}",
        f"--terms-csv={terms_csv}",
    ]
    print(f"Running import for entry_group={entry_group} glossary={glossary} ...")
    proc = subprocess.run(cmd, env=env, capture_output=True, text=True, timeout=timeout, input="y\n")
    success = proc.returncode == 0
    print(f"Return code: {proc.returncode}")
    if proc.stdout:
        print("--- STDOUT ---")
        print(proc.stdout.strip())
    if proc.stderr:
        print("--- STDERR ---")
        print(proc.stderr.strip())
    return success, proc.returncode

def main(args=None):
    parser = argparse.ArgumentParser(description="Import glossaries from CSVs.")
    parser.add_argument("--output-root", default=DEFAULT_OUTPUT_ROOT, help="Output root directory")
    parser.add_argument("--project", default=DEFAULT_PROJECT, help="GCP project id")
    parser.add_argument("--location", default=DEFAULT_LOCATION, help="Location")
    parser.add_argument("--max-threads", type=int, default=50, help="Max parallel threads (default: 50)")
    parsed_args = parser.parse_args(args)

    output_root = Path(parsed_args.output_root)
    created_file = output_root / "_created_glossaries.txt"
    if not created_file.exists():
        print(f"Created glossaries mapping file not found: {created_file}", file=sys.stderr)
        sys.exit(1)

    lines = [line.strip() for line in created_file.read_text(encoding="utf-8").splitlines() if line.strip()]
    mappings = []
    for line in lines:
        if "," not in line:
            print(f"Invalid mapping line (expected 'entry_group,glossary'): {line}", file=sys.stderr)
            continue
        entry_group, glossary = [p.strip() for p in line.split(",", 1)]
        mappings.append((entry_group, glossary))

    if not mappings:
        print("No valid mappings found in created file.", file=sys.stderr)
        sys.exit(1)

    summary = {"success": [], "failed": []}
    
    # Use ThreadPoolExecutor for parallel imports
    print(f"Importing {len(mappings)} glossaries in parallel (max {parsed_args.max_threads} threads)...")
    with ThreadPoolExecutor(max_workers=parsed_args.max_threads) as executor:
        futures = {}
        for g in lines:
            entry_group, glossary = [p.strip() for p in g.split(",", 1)]
            categories = output_root / g / "categories.csv"
            terms = output_root / g / "terms.csv"
            
            if not categories.exists() or not terms.exists():
                print(f"Skipping {glossary} (entry_group={entry_group}): missing files", file=sys.stderr)
                summary["failed"].append((glossary, "missing_files"))
                continue
            
            # Submit import task to thread pool
            future = executor.submit(
                run_import_for_glossary, 
                parsed_args.project, 
                parsed_args.location, 
                entry_group, 
                glossary, 
                str(categories), 
                str(terms)
            )
            futures[future] = (entry_group, glossary)
        
        # Collect results as they complete
        for future in as_completed(futures):
            entry_group, glossary = futures[future]
            try:
                ok, code = future.result()
                if ok:
                    summary["success"].append((entry_group, glossary))
                else:
                    summary["failed"].append((entry_group, glossary, f"exit_{code}"))
            except Exception as e:
                print(f"Exception during import of {glossary}: {e}", file=sys.stderr)
                summary["failed"].append((entry_group, glossary, str(e)))

    print("\nIMPORT SUMMARY")
    print("Succeeded:", len(summary["success"]))
    print("Failed:", len(summary["failed"]))
    if summary["failed"]:
        for f in summary["failed"][:20]:
            print(" -", f)
        if args is None:
            sys.exit(2)
    elif args is None:
        sys.exit(0)

if __name__ == "__main__":
    main()
